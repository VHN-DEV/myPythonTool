#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tool: T√¨m file tr√πng l·∫∑p d·ª±a tr√™n hash (MD5/SHA256)

M·ª•c ƒë√≠ch: Ph√°t hi·ªán v√† x√≥a file tr√πng l·∫∑p ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng
L√Ω do: D·ªçn d·∫πp ·ªï ƒëƒ©a, t·ªëi ∆∞u kh√¥ng gian l∆∞u tr·ªØ
"""

import os
import sys
import hashlib
import argparse
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed

# Th√™m th∆∞ m·ª•c cha v√†o sys.path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import (
    print_header, format_size, get_user_input, confirm_action,
    ProgressBar, log_info, log_error, setup_logger, safe_delete
)


def get_file_hash(file_path: str, hash_algo: str = 'md5', chunk_size: int = 8192) -> Optional[str]:
    """
    T√≠nh hash c·ªßa file
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file
        hash_algo: Thu·∫≠t to√°n hash (md5, sha1, sha256)
        chunk_size: K√≠ch th∆∞·ªõc chunk ƒë·ªçc file (bytes)
    
    Returns:
        str: Hash string ho·∫∑c None n·∫øu l·ªói
    
    Gi·∫£i th√≠ch:
    - ƒê·ªçc file theo chunk ƒë·ªÉ tr√°nh tr√†n RAM v·ªõi file l·ªõn
    - Hash t·ª´ng chunk v√† c·∫≠p nh·∫≠t v√†o hasher
    - Tr·∫£ v·ªÅ hex digest
    """
    # Ch·ªçn thu·∫≠t to√°n hash
    if hash_algo == 'md5':
        hasher = hashlib.md5()
    elif hash_algo == 'sha1':
        hasher = hashlib.sha1()
    elif hash_algo == 'sha256':
        hasher = hashlib.sha256()
    else:
        hasher = hashlib.md5()
    
    try:
        with open(file_path, 'rb') as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                hasher.update(chunk)
        return hasher.hexdigest()
    except (IOError, OSError, PermissionError) as e:
        log_error(f"L·ªói ƒë·ªçc file {file_path}: {e}")
        return None


def get_file_hash_with_size(args: Tuple[str, int, str]) -> Tuple[str, Optional[str], int]:
    """
    Wrapper function cho multiprocessing
    
    Args:
        args: tuple (file_path, size, hash_algo)
    
    Returns:
        tuple: (file_path, hash, size)
    
    Gi·∫£i th√≠ch:
    - Wrapper ƒë·ªÉ d√πng v·ªõi ProcessPoolExecutor
    - Tr·∫£ v·ªÅ c·∫£ path v√† hash v√† size
    """
    file_path, size, hash_algo = args
    file_hash = get_file_hash(file_path, hash_algo)
    return file_path, file_hash, size


class DuplicateFinder:
    """
    Class t√¨m file tr√πng l·∫∑p
    
    M·ª•c ƒë√≠ch: T·∫≠p trung logic t√¨m duplicate, d·ªÖ m·ªü r·ªông
    """
    
    def __init__(self, folder_path: str, recursive: bool = True, 
                 min_size: int = 0, hash_algo: str = 'md5'):
        """
        Kh·ªüi t·∫°o DuplicateFinder
        
        Args:
            folder_path: Th∆∞ m·ª•c c·∫ßn qu√©t
            recursive: C√≥ qu√©t th∆∞ m·ª•c con kh√¥ng
            min_size: K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu (bytes)
            hash_algo: Thu·∫≠t to√°n hash
        """
        self.folder_path = Path(folder_path).resolve()
        self.recursive = recursive
        self.min_size = min_size
        self.hash_algo = hash_algo
        self.file_count = 0
    
    def find_by_size_first(self, use_multiprocessing: bool = True) -> Dict[str, List[Tuple[str, int]]]:
        """
        T√¨m duplicate b·∫±ng c√°ch filter theo size tr∆∞·ªõc
        
        Args:
            use_multiprocessing: C√≥ d√πng multiprocessing kh√¥ng
        
        Returns:
            dict: {hash: [(file_path, size), ...]}
        
        Gi·∫£i th√≠ch:
        - B∆∞·ªõc 1: Group files theo size
        - B∆∞·ªõc 2: Ch·ªâ hash c√°c file c√≥ size tr√πng nhau
        - T·ªëi ∆∞u h∆°n so v·ªõi hash t·∫•t c·∫£ file
        """
        log_info(f"B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c: {self.folder_path}")
        
        # B∆∞·ªõc 1: Group theo size
        print("üîç B∆∞·ªõc 1: Qu√©t v√† group theo k√≠ch th∆∞·ªõc...\n")
        size_dict = defaultdict(list)
        
        if self.recursive:
            for root, dirs, files in os.walk(self.folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        size = os.path.getsize(file_path)
                        
                        if size < self.min_size:
                            continue
                        
                        size_dict[size].append(file_path)
                        self.file_count += 1
                        
                        if self.file_count % 100 == 0:
                            print(f"   ƒê√£ qu√©t {self.file_count} file...", end='\r')
                    
                    except (OSError, PermissionError):
                        pass
        else:
            for file in os.listdir(self.folder_path):
                file_path = os.path.join(self.folder_path, file)
                if os.path.isfile(file_path):
                    try:
                        size = os.path.getsize(file_path)
                        
                        if size < self.min_size:
                            continue
                        
                        size_dict[size].append(file_path)
                        self.file_count += 1
                    
                    except (OSError, PermissionError):
                        pass
        
        print(f"   ƒê√£ qu√©t {self.file_count} file.       ")
        
        # L·ªçc ch·ªâ l·∫•y size c√≥ nhi·ªÅu h∆°n 1 file (potential duplicates)
        potential_duplicates = {s: files for s, files in size_dict.items() if len(files) > 1}
        
        if not potential_duplicates:
            print("\n‚úÖ Kh√¥ng c√≥ file n√†o c√≥ c√πng k√≠ch th∆∞·ªõc!")
            return {}
        
        # ƒê·∫øm s·ªë file c·∫ßn hash
        files_to_hash = sum(len(files) for files in potential_duplicates.values())
        print(f"\nüîç B∆∞·ªõc 2: T√¨m th·∫•y {files_to_hash} file c√≥ c√πng k√≠ch th∆∞·ªõc")
        print(f"   ƒêang t√≠nh hash...\n")
        
        log_info(f"T√¨m th·∫•y {files_to_hash} file potential duplicate")
        
        # B∆∞·ªõc 2: Hash c√°c file c√≥ c√πng size
        hash_dict = defaultdict(list)
        
        if use_multiprocessing and files_to_hash > 5:
            # S·ª≠ d·ª•ng multiprocessing
            progress = ProgressBar(files_to_hash, prefix="T√≠nh hash:")
            
            # Chu·∫©n b·ªã tasks
            tasks = []
            for size, file_paths in potential_duplicates.items():
                for file_path in file_paths:
                    tasks.append((file_path, size, self.hash_algo))
            
            # X·ª≠ l√Ω song song
            import multiprocessing
            max_workers = min(multiprocessing.cpu_count(), files_to_hash)
            
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(get_file_hash_with_size, task) for task in tasks]
                
                for future in as_completed(futures):
                    try:
                        file_path, file_hash, size = future.result()
                        
                        if file_hash:
                            hash_dict[file_hash].append((file_path, size))
                        
                        progress.update()
                    
                    except Exception as e:
                        log_error(f"L·ªói khi hash: {e}")
                        progress.update()
            
            progress.finish()
        
        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª±
            progress = ProgressBar(files_to_hash, prefix="T√≠nh hash:")
            
            for size, file_paths in potential_duplicates.items():
                for file_path in file_paths:
                    file_hash = get_file_hash(file_path, self.hash_algo)
                    if file_hash:
                        hash_dict[file_hash].append((file_path, size))
                    progress.update()
            
            progress.finish()
        
        # L·ªçc ch·ªâ l·∫•y hash c√≥ nhi·ªÅu h∆°n 1 file (th·ª±c s·ª± duplicate)
        duplicates = {h: files for h, files in hash_dict.items() if len(files) > 1}
        
        log_info(f"T√¨m th·∫•y {len(duplicates)} nh√≥m file tr√πng l·∫∑p")
        
        return duplicates
    
    def find_by_size_only(self) -> Dict[int, List[str]]:
        """
        T√¨m duplicate ch·ªâ d·ª±a v√†o size (nhanh nh∆∞ng kh√¥ng ch√≠nh x√°c)
        
        Returns:
            dict: {size: [file_paths]}
        
        Gi·∫£i th√≠ch:
        - Ch·ªâ so s√°nh size, kh√¥ng hash
        - Nhanh nh∆∞ng c√≥ th·ªÉ false positive
        - H·ªØu √≠ch cho quick scan
        """
        print("üîç ƒêang qu√©t file theo k√≠ch th∆∞·ªõc...\n")
        log_info("Qu√©t theo size only")
        
        size_dict = defaultdict(list)
        
        if self.recursive:
            for root, dirs, files in os.walk(self.folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        size = os.path.getsize(file_path)
                        
                        if size < self.min_size:
                            continue
                        
                        size_dict[size].append(file_path)
                        self.file_count += 1
                        
                        if self.file_count % 100 == 0:
                            print(f"   ƒê√£ qu√©t {self.file_count} file...", end='\r')
                    
                    except (OSError, PermissionError):
                        pass
        else:
            for file in os.listdir(self.folder_path):
                file_path = os.path.join(self.folder_path, file)
                if os.path.isfile(file_path):
                    try:
                        size = os.path.getsize(file_path)
                        
                        if size < self.min_size:
                            continue
                        
                        size_dict[size].append(file_path)
                        self.file_count += 1
                    
                    except (OSError, PermissionError):
                        pass
        
        print(f"   ƒê√£ qu√©t {self.file_count} file.       ")
        
        # L·ªçc ch·ªâ l·∫•y size c√≥ nhi·ªÅu h∆°n 1 file
        duplicates = {s: files for s, files in size_dict.items() if len(files) > 1}
        
        return duplicates


def display_duplicates(duplicates: dict, by_hash: bool = True, limit: int = 20) -> Tuple[int, int]:
    """
    Hi·ªÉn th·ªã danh s√°ch file tr√πng l·∫∑p
    
    Args:
        duplicates: Dictionary ch·ª©a duplicates
        by_hash: True = duplicates theo hash, False = theo size
        limit: S·ªë nh√≥m t·ªëi ƒëa hi·ªÉn th·ªã
    
    Returns:
        tuple: (total_duplicates, wasted_space)
    
    Gi·∫£i th√≠ch:
    - Hi·ªÉn th·ªã t·ª´ng nh√≥m duplicate
    - T√≠nh to√°n dung l∆∞·ª£ng l√£ng ph√≠
    - Gi·ªõi h·∫°n s·ªë nh√≥m hi·ªÉn th·ªã ƒë·ªÉ kh√¥ng qu√° d√†i
    """
    if not duplicates:
        print("\n‚úÖ Kh√¥ng t√¨m th·∫•y file tr√πng l·∫∑p!")
        return 0, 0
    
    duplicate_groups = len(duplicates)
    total_duplicates = sum(len(files) - 1 for files in duplicates.values())
    wasted_space = 0
    
    print(f"\n{'='*60}")
    print(f"üìä T√¨m th·∫•y {duplicate_groups} nh√≥m file tr√πng l·∫∑p")
    print(f"{'='*60}\n")
    
    log_info(f"T√¨m th·∫•y {duplicate_groups} nh√≥m, {total_duplicates} file duplicate")
    
    for idx, (key, files) in enumerate(list(duplicates.items())[:limit], 1):
        if by_hash:
            # files l√† list c·ªßa tuple (path, size)
            file_size = files[0][1]
            file_paths = [f[0] for f in files]
        else:
            # files l√† list c·ªßa path, key l√† size
            file_size = key
            file_paths = files
        
        group_waste = file_size * (len(file_paths) - 1)
        wasted_space += group_waste
        
        print(f"Nh√≥m {idx}: {len(file_paths)} file ({format_size(file_size)}) - L√£ng ph√≠: {format_size(group_waste)}")
        
        if by_hash:
            print(f"   Hash: {key[:16]}...")
        
        for file_path in file_paths:
            print(f"   - {file_path}")
        
        print()
    
    if len(duplicates) > limit:
        remaining = len(duplicates) - limit
        print(f"... v√† {remaining} nh√≥m kh√°c (d√πng -a ƒë·ªÉ hi·ªÉn th·ªã t·∫•t c·∫£)\n")
    
    return total_duplicates, wasted_space


def save_report(duplicates: dict, output_file: str, by_hash: bool = True):
    """
    L∆∞u b√°o c√°o file tr√πng l·∫∑p ra file
    
    Args:
        duplicates: Dictionary ch·ª©a duplicates
        output_file: T√™n file output
        by_hash: True = duplicates theo hash
    
    Gi·∫£i th√≠ch:
    - Xu·∫•t to√†n b·ªô k·∫øt qu·∫£ ra file text
    - D·ªÖ xem l·∫°i v√† ph√¢n t√≠ch sau
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("  B√ÅO C√ÅO FILE TR√ôNG L·∫∂P\n")
        f.write("=" * 60 + "\n\n")
        
        duplicate_groups = len(duplicates)
        total_duplicates = sum(len(files) - 1 for files in duplicates.values())
        
        f.write(f"T·ªïng s·ªë nh√≥m tr√πng l·∫∑p: {duplicate_groups}\n")
        f.write(f"T·ªïng s·ªë file tr√πng l·∫∑p: {total_duplicates}\n\n")
        
        for idx, (key, files) in enumerate(duplicates.items(), 1):
            if by_hash:
                file_size = files[0][1]
                file_paths = [f[0] for f in files]
            else:
                file_size = key
                file_paths = files
            
            group_waste = file_size * (len(file_paths) - 1)
            
            f.write(f"\nNh√≥m {idx}: {len(file_paths)} file ({format_size(file_size)})\n")
            if by_hash:
                f.write(f"Hash: {key}\n")
            
            for file_path in file_paths:
                f.write(f"  - {file_path}\n")
    
    log_info(f"ƒê√£ l∆∞u b√°o c√°o: {output_file}")


def delete_duplicates_interactive(duplicates: dict, by_hash: bool = True):
    """
    X√≥a file tr√πng l·∫∑p v·ªõi t∆∞∆°ng t√°c
    
    Args:
        duplicates: Dictionary ch·ª©a duplicates
        by_hash: True = duplicates theo hash
    
    Gi·∫£i th√≠ch:
    - Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn c√°ch x√≥a
    - X√°c nh·∫≠n tr∆∞·ªõc khi x√≥a
    - Logging t·ª´ng file b·ªã x√≥a
    """
    print("\n===== CH·∫æ ƒê·ªò X√ìA TR√ôNG L·∫∂P =====")
    print("1. Gi·ªØ file ƒë·∫ßu ti√™n, x√≥a c√°c file c√≤n l·∫°i")
    print("2. Gi·ªØ file m·ªõi nh·∫•t (theo modification time), x√≥a c≈© h∆°n")
    print("3. Gi·ªØ file c≈© nh·∫•t (theo modification time), x√≥a m·ªõi h∆°n")
    print("0. Kh√¥ng x√≥a")
    
    choice = get_user_input("Ch·ªçn (0-3)", default="0")
    
    if choice == "0":
        return
    
    files_to_delete = []
    
    for key, files in duplicates.items():
        if by_hash:
            file_paths = [f[0] for f in files]
        else:
            file_paths = files
        
        if choice == "1":
            # Gi·ªØ file ƒë·∫ßu ti√™n
            files_to_delete.extend(file_paths[1:])
        
        elif choice == "2":
            # Gi·ªØ file m·ªõi nh·∫•t
            files_with_mtime = [(f, os.path.getmtime(f)) for f in file_paths]
            files_sorted = sorted(files_with_mtime, key=lambda x: x[1], reverse=True)
            files_to_delete.extend([f[0] for f in files_sorted[1:]])
        
        elif choice == "3":
            # Gi·ªØ file c≈© nh·∫•t
            files_with_mtime = [(f, os.path.getmtime(f)) for f in file_paths]
            files_sorted = sorted(files_with_mtime, key=lambda x: x[1])
            files_to_delete.extend([f[0] for f in files_sorted[1:]])
    
    if not files_to_delete:
        print("Kh√¥ng c√≥ file n√†o ƒë·ªÉ x√≥a.")
        return
    
    # Hi·ªÉn th·ªã preview
    print(f"\n‚ö†Ô∏è  S·∫º X√ìA {len(files_to_delete)} FILE:")
    for i, file_path in enumerate(files_to_delete[:10], 1):
        print(f"   {i}. {file_path}")
    
    if len(files_to_delete) > 10:
        print(f"   ... v√† {len(files_to_delete) - 10} file kh√°c")
    
    if not confirm_action(f"B·∫°n s·∫Øp x√≥a {len(files_to_delete)} file!", require_yes=True):
        print("‚ùå ƒê√£ h·ªßy")
        return
    
    # X√≥a files
    print(f"\nüóëÔ∏è  ƒêang x√≥a...\n")
    progress = ProgressBar(len(files_to_delete), prefix="X√≥a file:")
    
    deleted = 0
    errors = 0
    
    for file_path in files_to_delete:
        success, error = safe_delete(file_path)
        
        if success:
            deleted += 1
            log_info(f"ƒê√£ x√≥a: {file_path}")
        else:
            errors += 1
            log_error(f"L·ªói x√≥a {file_path}: {error}")
        
        progress.update()
    
    progress.finish()
    
    print(f"\n‚úÖ ƒê√£ x√≥a {deleted}/{len(files_to_delete)} file")
    if errors > 0:
        print(f"‚ùå {errors} file g·∫∑p l·ªói")
    
    log_info(f"X√≥a ho√†n th√†nh: {deleted} th√†nh c√¥ng, {errors} l·ªói")


def main_interactive():
    """Ch·∫ø ƒë·ªô interactive"""
    print_header("TOOL T√åM FILE TR√ôNG L·∫∂P")
    
    # Nh·∫≠p th∆∞ m·ª•c
    folder_input = get_user_input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c")
    if not folder_input or not os.path.isdir(folder_input):
        print("‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i!")
        return
    
    # T√πy ch·ªçn
    recursive_input = get_user_input("T√¨m trong t·∫•t c·∫£ th∆∞ m·ª•c con? (Y/n)", default="y")
    recursive = recursive_input.lower() != 'n'
    
    min_size_input = get_user_input("K√≠ch th∆∞·ªõc file t·ªëi thi·ªÉu (KB, Enter ƒë·ªÉ b·ªè qua)", default="0")
    min_size = int(min_size_input) * 1024 if min_size_input.isdigit() else 0
    
    # Ch·ªçn ph∆∞∆°ng ph√°p
    print("\n===== PH∆Ø∆†NG PH√ÅP T√åM =====")
    print("1. Theo hash (MD5) - Ch√≠nh x√°c nh∆∞ng ch·∫≠m")
    print("2. Theo hash (SHA256) - Ch√≠nh x√°c h∆°n MD5")
    print("3. Theo k√≠ch th∆∞·ªõc - Nhanh nh∆∞ng kh√¥ng ch√≠nh x√°c")
    
    method = get_user_input("Ch·ªçn ph∆∞∆°ng ph√°p (1-3)", default="1")
    
    # Multiprocessing
    use_mp = get_user_input("S·ª≠ d·ª•ng multiprocessing? (Y/n)", default="y")
    use_multiprocessing = use_mp.lower() != 'n'
    
    # T·∫°o finder
    if method in ["1", "2"]:
        hash_algo = 'md5' if method == "1" else 'sha256'
        finder = DuplicateFinder(folder_input, recursive, min_size, hash_algo)
        duplicates = finder.find_by_size_first(use_multiprocessing)
        
        total_duplicates, wasted_space = display_duplicates(duplicates, by_hash=True)
        
        if duplicates:
            print(f"{'='*60}")
            print(f"üíæ T·ªïng dung l∆∞·ª£ng l√£ng ph√≠: {format_size(wasted_space)}")
            print(f"{'='*60}")
    
    elif method == "3":
        finder = DuplicateFinder(folder_input, recursive, min_size)
        duplicates = finder.find_by_size_only()
        
        total_duplicates, wasted_space = display_duplicates(duplicates, by_hash=False)
        
        if duplicates:
            print(f"{'='*60}")
            print(f"üíæ ∆Ø·ªõc t√≠nh dung l∆∞·ª£ng l√£ng ph√≠: {format_size(wasted_space)}")
            print(f"‚ö†Ô∏è  L∆∞u √Ω: Ph∆∞∆°ng ph√°p n√†y ch·ªâ d·ª±a tr√™n k√≠ch th∆∞·ªõc, c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%")
            print(f"{'='*60}")
    
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
        return
    
    if not duplicates:
        return
    
    # L∆∞u b√°o c√°o
    save_input = get_user_input("\nL∆∞u b√°o c√°o ra file? (y/N)", default="n")
    if save_input.lower() == 'y':
        output_file = "duplicate_report.txt"
        save_report(duplicates, output_file, by_hash=(method in ["1", "2"]))
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o: {output_file}")
    
    # X√≥a file tr√πng l·∫∑p
    delete_input = get_user_input("\nX√≥a file tr√πng l·∫∑p? (y/N)", default="n")
    if delete_input.lower() == 'y':
        delete_duplicates_interactive(duplicates, by_hash=(method in ["1", "2"]))


def main_cli(args):
    """Ch·∫ø ƒë·ªô CLI"""
    hash_algo = 'sha256' if args.sha256 else 'md5'
    
    finder = DuplicateFinder(
        args.directory,
        recursive=not args.no_recursive,
        min_size=args.min_size * 1024 if args.min_size else 0,
        hash_algo=hash_algo
    )
    
    if args.size_only:
        duplicates = finder.find_by_size_only()
        by_hash = False
    else:
        duplicates = finder.find_by_size_first(not args.no_multiprocessing)
        by_hash = True
    
    display_duplicates(duplicates, by_hash, limit=1000 if args.all else 20)
    
    if args.output:
        save_report(duplicates, args.output, by_hash)
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o: {args.output}")


def main():
    """H√†m main"""
    setup_logger('duplicate-finder', log_to_console=False)
    
    parser = argparse.ArgumentParser(description='Tool t√¨m file tr√πng l·∫∑p')
    parser.add_argument('directory', nargs='?', help='Th∆∞ m·ª•c c·∫ßn qu√©t')
    parser.add_argument('--sha256', action='store_true', help='D√πng SHA256 thay v√¨ MD5')
    parser.add_argument('--size-only', action='store_true', help='Ch·ªâ so s√°nh theo size')
    parser.add_argument('--min-size', type=int, help='K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu (KB)')
    parser.add_argument('--no-recursive', action='store_true', help='Kh√¥ng qu√©t th∆∞ m·ª•c con')
    parser.add_argument('--no-multiprocessing', action='store_true', help='T·∫Øt multiprocessing')
    parser.add_argument('-o', '--output', help='File output cho b√°o c√°o')
    parser.add_argument('-a', '--all', action='store_true', help='Hi·ªÉn th·ªã t·∫•t c·∫£ k·∫øt qu·∫£')
    
    args = parser.parse_args()
    
    if args.directory:
        main_cli(args)
    else:
        try:
            main_interactive()
        except KeyboardInterrupt:
            print("\n\n‚ùå ƒê√£ h·ªßy!")
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}")
            log_error(f"Exception: {e}", exc_info=True)


if __name__ == "__main__":
    main()
